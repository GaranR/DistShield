{
 "cells": [
  {
   "cell_type": "code",
   "id": "627727730b2cb354",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW, AutoModelForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "import timeout_decorator"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from transformers import pipeline\n",
    "# \n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "# ]\n",
    "# pipe = pipeline(\"text-generation\", model=\"Creekside/Qwen-3B-gsm8k-GRPO\")\n",
    "# pipe(messages)"
   ],
   "id": "777eb0ad31dcd561",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def draw(weights, name, mask):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bin_width = 0.005\n",
    "    bins = np.arange(-0.6, 0.6, bin_width)\n",
    "    plt.hist(weights.view(-1).cpu(), bins=bins, alpha=0.75, color='blue', edgecolor='black')\n",
    "    plt.hist(weights[mask[name + '.weight'].to('cpu') > 0].view(-1).cpu(), bins=bins, alpha=0.75, color='red', edgecolor='black')\n",
    "    plt.title(name)\n",
    "    plt.xlim([-0.6, 0.6])\n",
    "    plt.ylim([0, 300])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    # plt.savefig(f\"imgs/{name}.png\")\n",
    "    # plt.close()\n",
    "    return"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "# import tqdm\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Creekside/Qwen-3B-gsm8k-GRPO\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Creekside/Qwen-3B-gsm8k-GRPO\", torch_dtype=torch.float16)"
   ],
   "id": "53917e1cc2393c30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(model)",
   "id": "a452503e083f3c33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"GSM8K\",\"main\")['test']\n",
    "print(dataset)"
   ],
   "id": "4dea18c16b7d49f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_max_length(example):\n",
    "    question = example['question']\n",
    "    answer = example['answer']\n",
    "    messages = [\n",
    "        { \"role\" : \"user\" , \"content\" : '{} Please think step by step, give the final number in a new line after #### without any other words.'.format(question)},\n",
    "        { \"role\" : \"assistant\" , \"content\" : '{}'.format(answer)}\n",
    "    ]\n",
    "    text = tokenizer.decode(tokenizer.apply_chat_template(messages))\n",
    "    tokens = tokenizer(text, padding=False, truncation=False)  \n",
    "    return {\"length\": len(tokens[\"input_ids\"])}\n",
    "\n",
    "lengths = dataset.map(get_max_length, batched=False)[\"length\"]\n",
    "max_length = max(lengths)\n",
    "del lengths\n",
    "print(f\"Max length in dataset: {max_length}\")"
   ],
   "id": "48a666f10921d461",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tokenize_function(example):\n",
    "    question = example['question']\n",
    "    answer = example['answer']\n",
    "    label_messages = [\n",
    "        { \"role\" : \"user\" , \"content\" : '{} Please think step by step, give the final number in a new line after #### without any other words.'.format(question)},\n",
    "        { \"role\" : \"assistant\" , \"content\" : '{}'.format(answer)}\n",
    "    ]\n",
    "    input_messages = [\n",
    "        { \"role\" : \"user\" , \"content\" : '{} Please think step by step, give the final number in a new line after #### without any other words.'.format(question)}\n",
    "    ]\n",
    "\n",
    "    label_text = tokenizer.decode(tokenizer.apply_chat_template(label_messages))\n",
    "    input_text = tokenizer.decode(tokenizer.apply_chat_template(input_messages))\n",
    "\n",
    "    tokenized_res = tokenizer(input_text, padding = 'max_length', max_length=max_length)\n",
    "\n",
    "    tokenized_res['labels'] = tokenizer(label_text, padding = 'max_length', max_length=max_length)['input_ids']\n",
    "    return tokenized_res\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function)"
   ],
   "id": "f3c01f8550fbcf29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenized_dataset.set_format('pt')\n",
    "print(tokenized_dataset)\n",
    "print(tokenized_dataset[0]['input_ids'].shape)\n",
    "print(tokenized_dataset[1]['input_ids'].shape)\n",
    "print(tokenized_dataset[1]['labels'].shape)\n",
    "print(tokenizer.decode(tokenized_dataset[1]['input_ids']))\n",
    "print(tokenizer.decode(tokenized_dataset[1]['labels']))"
   ],
   "id": "815955d28c28d199",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_size = 1\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=batch_size)\n",
    "\n",
    "my_dataloader = DataLoader(tokenized_dataset.select(range(50)), batch_size=batch_size)"
   ],
   "id": "2e249768b4a1c41e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# import accelerate",
   "id": "be48966ee50fdc3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.train()\n",
    "model = model.to('cuda')\n",
    "model.zero_grad()\n",
    "progress_bar = tqdm(range(len(dataset)))\n",
    "\n",
    "# accelerator = accelerate.Accelerator()\n",
    "# model, dataloader = accelerator.prepare(model, dataloader)\n",
    "\n",
    "for batch in dataloader:\n",
    "\n",
    "    input_ids = batch['input_ids'].to('cuda')\n",
    "    label = batch['labels'].to('cuda')\n",
    "    attention_mask = batch['attention_mask'].to('cuda')\n",
    "\n",
    "    output = model(input_ids=input_ids, attention_mask=attention_mask, labels=label, return_dict=True)\n",
    "    print(output.loss)\n",
    "    output.loss.backward()\n",
    "    # accelerator.backward(output.loss)\n",
    "    print(output)\n",
    "    # answer = tokenizer.decode(output[0]) # only the first in batch\n",
    "    # print(answer)\n",
    "\n",
    "    break"
   ],
   "id": "4651c5a8c097ed88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ori_dict = {}\n",
    "ori_max = {}\n",
    "ori_min = {}\n",
    "for name, module in model.named_modules():\n",
    "    if 'k_proj' in name or 'q_proj' in name or 'v_proj' in name:\n",
    "        name = name + '.weight'\n",
    "        ori_dict[name] = module.weight.clone().to('cpu')\n",
    "        flattened_para = module.weight.view(-1)\n",
    "        sorted_tensor, _ = torch.sort(flattened_para)\n",
    "        num_elements = flattened_para.numel()\n",
    "        top_1_percent_idx = num_elements - 50\n",
    "        bottom_1_percent_idx = 50\n",
    "        top_1_percent_value = sorted_tensor[top_1_percent_idx].item()\n",
    "        bottom_1_percent_value = sorted_tensor[bottom_1_percent_idx].item()\n",
    "        ori_max[name] = top_1_percent_value\n",
    "        ori_min[name] = bottom_1_percent_value"
   ],
   "id": "6e7f34e52f14f797",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mask = {}\n",
    "idx_list = []\n",
    "ori_w = []\n",
    "ratio = 0.001\n",
    "for name, module in model.named_modules():\n",
    "    if 'k_proj' in name or 'q_proj' in name or 'v_proj' in name:\n",
    "        weight_name = name + '.weight'\n",
    "        weights = module.weight.data\n",
    "        \n",
    "        flattened_weights = weights.view(-1)\n",
    "        replace_count = int(ratio * len(flattened_weights))\n",
    "        replace_count = max(0, replace_count)\n",
    "        print(weight_name)\n",
    "        print(replace_count)\n",
    "        mask[weight_name] = torch.zeros_like(flattened_weights)\n",
    "        \n",
    "        if module.weight.grad is None:\n",
    "            raise ValueError(f\"{name}\")\n",
    "        grad_abs = module.weight.grad.detach().abs()\n",
    "        _, topk_idx = grad_abs.view(-1).topk(replace_count)\n",
    "        mask[weight_name][topk_idx] = 1\n",
    "        print((mask[weight_name] > 0).sum())\n",
    "        mask[weight_name] = mask[weight_name].view(weights.size()).to('cpu')\n",
    "        \n",
    "        indexes = torch.nonzero(mask[weight_name], as_tuple=True)\n",
    "        idx_list.append(indexes)\n",
    "        selected_weights = weights[indexes]\n",
    "        ori_w.extend(selected_weights.cpu().detach().numpy().flatten())\n",
    "\n",
    "print(\"mask include:\", mask.keys())"
   ],
   "id": "95a39f2fc2c4923a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3d2ce6cd93a5b43c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print((mask['model.layers.0.self_attn.q_proj.weight'] > 0).to('cpu').sum())",
   "id": "88b6702e67b38da8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# optimizer = AdamW(model.parameters(), lr=0.00005, eps=1e-8)\n",
    "from transformers.optimization import Adafactor\n",
    "optimizer = Adafactor(model.parameters(), lr=0.005, relative_step=False)\n",
    "device = 'cuda'\n",
    "for epoch in range(10):  # Specify how many epochs to cycle through the training\n",
    "    # set to the eval mode to fix the paramaters of batchnorm\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    progress_bar = tqdm(my_dataloader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()\n",
    "        model.to('cuda')\n",
    "        input_ids = batch['input_ids'].to('cuda')\n",
    "        label = batch['labels'].to('cuda')\n",
    "        attention_mask = batch['attention_mask'].to('cuda')\n",
    "    \n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask, labels=label, return_dict=True)\n",
    "        loss = output.loss\n",
    "        # print(loss)\n",
    "        \n",
    "        kld = 0\n",
    "        cnt = 0\n",
    "        total_cnt = 0\n",
    "        for name, para in model.named_parameters():\n",
    "            if name in mask:\n",
    "                flattened_para = para[mask[name].to('cuda') > 0].view(-1).to('cpu')\n",
    "                flattened_ori = ori_dict[name][mask[name] > 0].view(-1).to('cpu')\n",
    "                flattened_para = flattened_para[torch.argsort(flattened_para)]\n",
    "                flattened_ori = flattened_ori[torch.argsort(flattened_ori)]\n",
    "                # print(flattened_ori)\n",
    "                cnt += 1\n",
    "                if flattened_para.numel() != 0:\n",
    "                    kl_t1 = torch.log_softmax(flattened_para, dim=0)\n",
    "                    kl_t2 = torch.softmax(flattened_ori, dim=0)\n",
    "                    kld_tmp = torch.nn.functional.kl_div(kl_t1, kl_t2, reduction='sum')\n",
    "                    # print(kld_tmp)\n",
    "                    if kld_tmp.item() > 0.0:\n",
    "                        kld += kld_tmp.item()\n",
    "        loss_new = -loss + 2 ** (kld * 5e1)\n",
    "        total_loss += loss_new.item()\n",
    "        # print(loss_new.item())\n",
    "        loss_new.backward()\n",
    "        # loss = loss * 0.0001\n",
    "        # loss.backward()\n",
    "        for name, para in model.named_parameters():\n",
    "            if name in mask:\n",
    "                para.grad *= mask[name].long().to('cuda')\n",
    "            else:\n",
    "                para.grad *= 0\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            for name, para in model.named_parameters():\n",
    "                if name in mask:\n",
    "                    para.data = (1 - mask[name].to('cuda')) * para + (mask[name].to('cuda') * para).clamp_(ori_min[name] * 0.85,\n",
    "                                                                                         ori_max[name] * 0.75)\n",
    "                   \n",
    "        progress_bar.set_postfix({'loss': loss.item(),'loss_new': loss_new.item(),'kld': 2 ** (kld * 5e1)})\n",
    "    break\n",
    "    # correct = 0\n",
    "    # total = 0\n",
    "    # with torch.no_grad():\n",
    "    #     for i in tqdm(range(10570)):\n",
    "    #         result = question_answerer(question=dataset['validation'][i]['question'], context=dataset['validation'][i]['context'])\n",
    "    #         for answer in dataset['validation'][i]['answers']['text']:\n",
    "    #             if answer == result['answer']:\n",
    "    #                 correct += 1\n",
    "    #                 break\n",
    "    #         total += 1\n",
    "    # accuracy = correct / total\n",
    "    # print(\"accuracy:\", accuracy)\n",
    "    # \n",
    "    # if accuracy > 0.7:\n",
    "    #     continue\n",
    "    changed = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if (name + '.weight') in mask:\n",
    "            changed += (mask[name + '.weight'] > 0).sum()\n",
    "        if 'v_proj' in name:\n",
    "            if 'layers.6' in name:\n",
    "                draw(module.weight.data, name, mask.to('cpu'))\n",
    "                # draw(ori_dict[name + '.weight'].data, name, mask)\n",
    "                name = name + '.weight'\n",
    "                flattened_weights = module.weight.data.cpu().numpy().flatten()\n",
    "                flattened_ori = ori_dict[name].data.cpu().numpy().flatten()\n",
    "                weight_diff = flattened_weights - flattened_ori\n",
    "                changed_indices = np.where(np.abs(weight_diff) > 1e-5)\n",
    "                print(len(changed_indices[0]))\n",
    "    # \n",
    "    print(f'{changed} weights changed')\n",
    "    if changed < 8500:\n",
    "        break\n",
    "    # if accuracy > 0.01:\n",
    "    #     continue\n",
    "    step = 0.1\n",
    "    with torch.no_grad():\n",
    "        for name, module in model.named_modules():\n",
    "            name = name + '.weight'\n",
    "            if 'k_proj' in name or 'q_proj' in name or 'v_proj' in name:\n",
    "                weights = module.weight.data\n",
    "                # flattened_weights = weights.view(-1)\n",
    "                replace_count = int(step * mask[name].to('cpu').sum())\n",
    "                # print((module.weight.grad.detach().abs() * mask[name]).view(-1).shape)\n",
    "                if replace_count == 0:\n",
    "                    replace_count = 1\n",
    "                temp_weight = module.weight.grad.detach().abs() * mask[name].to('cpu')\n",
    "                temp_weight += 1 - mask[name].to('cpu')\n",
    "                temp_weight = temp_weight.view(-1)\n",
    "                _, w_idx_mink = temp_weight.topk(replace_count, largest=False)\n",
    "                zero_mask = torch.zeros_like(temp_weight)\n",
    "                zero_mask[w_idx_mink] = 1\n",
    "                # if 'layer2.1.conv1' in name or 'features.4' in name:\n",
    "                #     print((weights[mask[name] > 0]).numel())\n",
    "                #     print((weights.view(-1) != ori_dict[name].view(-1)).sum().item())\n",
    "                    # print(weights[mask[name] > 0])\n",
    "                    # print(ori_dict[name][mask[name] > 0])\n",
    "                    # mask_temp = mask[name].clone()\n",
    "                mask[name][zero_mask.view(mask[name].size()) > 0] = 0\n",
    "                weights[zero_mask.view(mask[name].to('cpu').size()) > 0] = ori_dict[name][zero_mask.view(mask[name].to('cpu').size()) > 0]\n",
    "                # if 'layer2.1.conv1' in name:\n",
    "                    # print(weights[mask_temp > 0])\n",
    "                    # print(ori_dict[name][mask_temp > 0])\n",
    "                    # print((weights[mask[name] > 0]).numel())\n",
    "                    # print((weights.view(-1) != ori_dict[name].view(-1)).sum().item())\n",
    "    optimizer = AdamW(model.parameters(), lr=0.0005, eps=1e-8)"
   ],
   "id": "e99824453360b557",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "aef39faf98d13783",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "51c022d2f3ce4cb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "changed = 0\n",
    "for name, module in model.named_modules():\n",
    "    if (name + '.weight') in mask:\n",
    "        changed += (mask[name + '.weight'] > 0).sum()\n",
    "    if 'v_proj' in name:\n",
    "        if 'layers.6' in name:\n",
    "            draw(module.weight.data, name, mask)\n",
    "            # draw(ori_dict[name + '.weight'].data, name, mask)\n",
    "            name = name + '.weight'\n",
    "            flattened_weights = module.weight.data.cpu().numpy().flatten()\n",
    "            flattened_ori = ori_dict[name].data.cpu().numpy().flatten()\n",
    "            weight_diff = flattened_weights - flattened_ori\n",
    "            changed_indices = np.where(np.abs(weight_diff) > 1e-10)\n",
    "            print(len(changed_indices[0]))\n",
    "# \n",
    "print(f'{changed} weights changed')"
   ],
   "id": "3ab5cb90d4d5bf67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# test_dataloader = DataLoader(\n",
    "#     dataset['test'],\n",
    "#     batch_size=1,  \n",
    "#     shuffle=False\n",
    "# )\n",
    "# \n",
    "# \n",
    "# \n",
    "# model.eval() \n",
    "# cnt = 0\n",
    "# with torch.no_grad():\n",
    "#     for data in test_dataloader:\n",
    "#         data = {k: v.to(model.device) for k, v in data.items() if k != 'question' and k != 'answer'}\n",
    "#         print(data)\n",
    "#         outputs = model(\n",
    "#             input_ids=data[\"input_ids\"],\n",
    "#             attention_mask=data[\"attention_mask\"],\n",
    "#             labels=data[\"labels\"]\n",
    "#         )\n",
    "#         \n",
    "#         print(\"\\n=== Test Sample ===\")\n",
    "#         print(outputs)\n",
    "#         print(f\"\\nModel Loss: {outputs.loss.item():.4f}\")\n",
    "#         \n",
    "#         generated = model.generate(\n",
    "#             input_ids=data[\"input_ids\"],\n",
    "#             attention_mask=data[\"attention_mask\"],\n",
    "#             max_new_tokens=100\n",
    "#         )\n",
    "#         print(\"\\nGenerated Response:\")\n",
    "#         print(tokenizer.decode(generated[0], skip_special_tokens=True))\n",
    "#         cnt += 1\n",
    "#         # if cnt ==3:\n",
    "#         break "
   ],
   "id": "45bb2ba752261381",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def extract_after_double_hash(text):\n",
    "#     pos = text.find(\"#### \")\n",
    "#     if pos != -1:\n",
    "#         return text[pos + 5:]  \n",
    "#     return None  \n",
    "# \n",
    "# \n",
    "# \n",
    "# idx = 675\n",
    "# \n",
    "# print(dataset['train'][idx]['question'])\n",
    "# print(dataset['train'][idx]['answer'])\n",
    "# question = dataset['train'][idx]['question']\n",
    "# answer = dataset['train'][idx]['answer']\n",
    "# prompt = f\"\"\"please anwser this math problem, think step by step, and give the final answer after \"#### \" in a new line: \n",
    "# \n",
    "# {question}\"\"\"\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": prompt},\n",
    "# ]\n",
    "# result = pipe(messages)\n",
    "# print(result[0]['generated_text'][1]['content'])\n",
    "# result = extract_after_double_hash(result[0]['generated_text'][1]['content'])\n",
    "# answer = extract_after_double_hash(answer)\n",
    "# print(result)  # 输出: \"72\"\n",
    "# print(answer)"
   ],
   "id": "3244d78324e8cc84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for batch in dataloader:\n",
    "\n",
    "    input_ids = batch['input_ids'].to('cuda')\n",
    "    label = batch['labels'].to('cuda')\n",
    "    attention_mask = batch['attention_mask'].to('cuda')\n",
    "    # print(input_ids)\n",
    "    output = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    # print(output)\n",
    "    # answer = tokenizer.decode(output[0])\n",
    "    # print(answer)\n",
    "    preds = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "    truths = tokenizer.batch_decode(label, skip_special_tokens=True)\n",
    "    \n",
    "    def extract_answer(text):\n",
    "        if \"####\" in text:\n",
    "            return text.split(\"####\")[-1].strip()\n",
    "        return \"\"\n",
    "    \n",
    "    for pred, truth in zip(preds, truths):\n",
    "        pred_answer = extract_answer(pred)\n",
    "        true_answer = extract_answer(truth)\n",
    "        if pred_answer == true_answer:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        print(f\"pred: {pred_answer}, truth: {true_answer}, acc: {correct / total}\")\n",
    "        if total == 10:\n",
    "            break\n",
    "    # output = model(input_ids=input_ids, attention_mask=attention_mask, labels=label, return_dict=True)\n",
    "    # print(output.loss)\n",
    "    # # output.loss.backward()\n",
    "    # # accelerator.backward(output.loss)\n",
    "    # print(output)\n",
    "    # answer = tokenizer.decode(output[0]) # only the first in batch\n",
    "    # print(answer)"
   ],
   "id": "161ccff5cca72a48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# timeout_seconds = 60\n",
    "# @timeout_decorator.timeout(timeout_seconds, timeout_exception=TimeoutError)\n",
    "# def process_question(pipe, prompt):\n",
    "#     return pipe([{\"role\": \"user\", \"content\": prompt}], max_new_tokens=5000)\n",
    "# \n",
    "# correct = 0\n",
    "# total = 0\n",
    "# progress_bar = tqdm(range(1319))\n",
    "# with torch.no_grad():\n",
    "#     for i in progress_bar:\n",
    "#         total += 1\n",
    "#         question = dataset['test'][i]['question']\n",
    "#         answer = dataset['test'][i]['answer']\n",
    "#         prompt = f\"\"\"please anwser this math problem, think step by step, and give the final answer after \"#### \" in a new line: \n",
    "# \n",
    "#         {question}\"\"\"\n",
    "#         try:\n",
    "#             result = process_question(pipe, prompt)\n",
    "#             if extract_after_double_hash(result[0]['generated_text'][1]['content']) == extract_after_double_hash(answer):\n",
    "#                 correct += 1\n",
    "#         except TimeoutError:\n",
    "#             print(f\"\\nTimeout on question {i}, skipping...\")\n",
    "#             continue\n",
    "#         except Exception as e:\n",
    "#             print(f\"\\nError on question {i}: {str(e)}, skipping...\")\n",
    "#             continue\n",
    "# \n",
    "#         progress_bar.set_postfix({'acc': correct / total})\n",
    "# \n",
    "# print(\"accuracy:\", correct / total)"
   ],
   "id": "82b039795563d0aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "def plot_weight_comparison_gpu(model, ori_dict, name):\n",
    "    device = next(model.parameters()).device\n",
    "    weights_post = model.state_dict()[name].detach()\n",
    "    weights_pre = ori_dict[name].detach().to(device)\n",
    "    \n",
    "    def gpu_hist(tensor, bins):\n",
    "        counts = torch.histc(tensor, bins=len(bins)-1, min=bins[0], max=bins[-1])\n",
    "        return counts.cpu().numpy(), bins\n",
    "    \n",
    "    bin_edges_gpu = torch.linspace(-0.3, 0.3, steps=121, device=device)\n",
    "    bin_edges = bin_edges_gpu.cpu().numpy()\n",
    "    bin_width = bin_edges[1] - bin_edges[0]\n",
    "    \n",
    "    pre_freq, _ = gpu_hist(weights_pre, bin_edges_gpu)\n",
    "    post_freq, _ = gpu_hist(weights_post, bin_edges_gpu)\n",
    "    \n",
    "    lim = max(int(max(pre_freq.max(), post_freq.max()) * 1.2), 100)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.bar(bin_edges[:-1], pre_freq, width=bin_width, align='edge',\n",
    "            alpha=0.5, color='royalblue', edgecolor='none', label='Original Weights')\n",
    "    \n",
    "    plt.bar(bin_edges[:-1], -post_freq, width=bin_width, align='edge',\n",
    "            alpha=0.5, color='coral', edgecolor='none', label='Current Weights')\n",
    "    \n",
    "    diff = torch.abs(weights_post - weights_pre)\n",
    "    significant = diff > 1e-10  \n",
    "    if significant.any():\n",
    "        sig_pre = weights_pre[significant]\n",
    "        sig_post = weights_post[significant]\n",
    "        sig_pre_freq, _ = gpu_hist(sig_pre, bin_edges_gpu)\n",
    "        sig_post_freq, _ = gpu_hist(sig_post, bin_edges_gpu)\n",
    "        \n",
    "        plt.bar(bin_edges[:-1], sig_pre_freq, width=bin_width, align='edge',\n",
    "                alpha=0.8, color='blue', edgecolor='none', label='Significant Change (Orig)')\n",
    "        plt.bar(bin_edges[:-1], -sig_post_freq, width=bin_width, align='edge',\n",
    "                alpha=0.8, color='red', edgecolor='none', label='Significant Change (Current)')\n",
    "    \n",
    "    plt.xlim(-0.3, 0.3)\n",
    "    lim = 20\n",
    "    plt.ylim(-lim, lim)\n",
    "    plt.grid(True, which='both', linestyle=':', alpha=0.5)\n",
    "    plt.axhline(0, color='k', linewidth=1)\n",
    "    plt.title(f'Weight Distribution Comparison: {name}', fontsize=14)\n",
    "    plt.xlabel('Weight Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{abs(x):.0f}'))\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_combined_weights_gpu(model, ori_dict):\n",
    "    weight_range = 0.3\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    all_weights_pre = torch.tensor([], device=device)\n",
    "    all_weights_post = torch.tensor([], device=device)\n",
    "    all_diffs = torch.tensor([], device=device)\n",
    "    \n",
    "    for name, param in model.state_dict().items():\n",
    "        if ('q_proj' in name or 'k_proj' in name or 'v_proj' in name) and 'weight' in name:\n",
    "            weights_post = param.detach().flatten()\n",
    "            weights_pre = ori_dict[name].detach().to(device).flatten()\n",
    "            diff = torch.abs(weights_post - weights_pre)\n",
    "            \n",
    "            all_weights_pre = torch.cat([all_weights_pre, weights_pre])\n",
    "            all_weights_post = torch.cat([all_weights_post, weights_post])\n",
    "            all_diffs = torch.cat([all_diffs, diff])\n",
    "    \n",
    "    bin_edges_gpu = torch.linspace(-weight_range, weight_range, steps=121, device=device)\n",
    "    \n",
    "    def gpu_hist(tensor):\n",
    "        return torch.histc(tensor, bins=120, min=-weight_range, max=weight_range).cpu().numpy()\n",
    "    \n",
    "    pre_freq = gpu_hist(all_weights_pre)\n",
    "    post_freq = gpu_hist(all_weights_post)\n",
    "    \n",
    "    diff = torch.abs(all_weights_post - all_weights_pre)\n",
    "    significant = diff > 1e-10\n",
    "    sig_pre_freq = gpu_hist(all_weights_pre[significant])\n",
    "    sig_post_freq = gpu_hist(all_weights_post[significant])\n",
    "    \n",
    "    bin_edges = bin_edges_gpu.cpu().numpy()\n",
    "    bin_width = bin_edges[1] - bin_edges[0]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.bar(bin_edges[:-1], pre_freq, width=bin_width, align='edge',\n",
    "            alpha=0.3, color='lightblue', edgecolor='black', \n",
    "            label='Baseline')\n",
    "    plt.bar(bin_edges[:-1], -post_freq, width=bin_width, align='edge',\n",
    "            alpha=0.2, color='red', edgecolor='black',\n",
    "            label='DistShield')\n",
    "    \n",
    "    # 绘制显著变化区域\n",
    "    plt.bar(bin_edges[:-1], sig_pre_freq, width=bin_width, align='edge',\n",
    "            alpha=0.6, color='blue', edgecolor='black',\n",
    "            label='Before')\n",
    "    plt.bar(bin_edges[:-1], -sig_post_freq, width=bin_width, align='edge',\n",
    "            alpha=0.6, color='red', edgecolor='black',\n",
    "            label='After')\n",
    "    \n",
    "    max_freq = max(pre_freq.max(), post_freq.max())\n",
    "    plt.xlim(-weight_range, weight_range)\n",
    "    lim = 26000\n",
    "    plt.ylim(-lim, lim)\n",
    "    plt.grid(True, which='both', linestyle=':', alpha=0.5)\n",
    "    plt.axhline(0, color='k', linewidth=1)\n",
    "    \n",
    "    total_params = len(all_weights_pre)\n",
    "    changed_params = significant.sum().item()\n",
    "    plt.title(\n",
    "        f'DistShield on Qwen-3B GSM8K', fontsize=20\n",
    "    )\n",
    "    \n",
    "    plt.xlabel('Weight Value', fontsize=20)\n",
    "    plt.ylabel('Frequency', fontsize=20)\n",
    "    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{abs(x):.0f}'))\n",
    "    plt.legend(loc='upper right', fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.savefig('./qwen_gsm8k.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    # plt.show()"
   ],
   "id": "758cf5fa6f416400",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_combined_weights_gpu(model, ori_dict)",
   "id": "e46f3e7e39f82261",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
